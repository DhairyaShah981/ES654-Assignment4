# -*- coding: utf-8 -*-
"""linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dSwJGtzoxAFZlW8itoyHW0NtJSze1Pk7
"""


from jax import grad
import jax.numpy as jnp
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import jax
import numpy as np
from sklearn.linear_model import LinearRegression
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
np.random.seed(45)


class LinearRegressionCustom():
    def __init__(self, fit_intercept=True, lasso_lambda=0.1, ridge_lambda=0.1, batch_size=15, lr=0.01, epochs=30, momentum=0.9):
        '''
        :param fit_intercept: Whether to calculate the intercept for this model.
        If set to False, no intercept will be used in calculations
        (i.e. data is expected to be centered).
        '''
        self.fit_intercept = fit_intercept
        self.coef_ = None  # Stores the coefficients learned using the fit methods
        # Stores the thetas for every iteration (theta vectors appended) (for the iterative methods)
        self.all_coef = []
        self.reg = None  # Stores the sklearn LinearRegression object
        self.X = None  # Stores the training data
        self.y = None  # Stores the training labels
        self.lasso_lambda = lasso_lambda  # Stores the lambda value for Lasso regression
        self.ridge_lambda = ridge_lambda  # Stores the lambda value for Ridge regression
        self.batch_size = batch_size
        self.lr = lr
        self.epochs = epochs
        self.momentum = momentum

    def fit_sklearn_LR(self, X, y):
        # Solve the linear regression problem by calling Linear Regression
        # from sklearn, with the relevant parameters
        self.X = X
        self.y = y
        if self.fit_intercept:
            ones_column = np.ones((self.X.shape[0], 1))
            self.X = np.concatenate((ones_column, self.X), axis=1)

        self.reg = LinearRegression(fit_intercept=self.fit_intercept)
        self.reg.fit(self.X, self.y)
        self.coef_ = self.reg.coef_

    def fit_normal_equations(self, X, y):
        # Solve the linear regression problem using the closed form solution
        # to the normal equation for minimizing ||Wx - y||_2^2
        self.X = X
        self.y = y
        if self.fit_intercept:
            ones_column = np.ones((self.X.shape[0], 1))
            self.X = np.concatenate((ones_column, self.X), axis=1)

        self.coef_ = np.linalg.inv(self.X.T.dot(self.X)).dot(self.X.T).dot(y)

    def fit_SVD(self, X, y):
        # Solve the linear regression problem using the SVD of the
        # coefficient matrix
        self.X = X
        self.y = y
        if self.fit_intercept:
            ones_column = np.ones((self.X.shape[0], 1))
            self.X = np.concatenate((ones_column, self.X), axis=1)

        U, s, Vt = np.linalg.svd(self.X)
        D = np.zeros((self.X.shape[0], self.X.shape[1]))
        D[:self.X.shape[1], :self.X.shape[1]] = np.diag(s)
        self.coef_ = Vt.T.dot(np.linalg.pinv(D)).dot(U.T).dot(y)

    def mse_loss(self):
        # Compute the MSE loss with the learned model
        y_pred = self.predict(self.X)
        mse = np.mean((self.y - y_pred)**2)
        return mse
        pass

    def predict(self, X):
        # Function to run the LinearRegression on a test data point
        if self.coef_ is None:
            raise Exception('Model has not been trained yet')
        self.X = np.array(self.X)
        return self.X.dot(self.coef_)

    def predict_kfold(self, X):
        # Function to run the LinearRegression on a test data point
        if(self.fit_intercept):
            ones_column = np.ones((X.shape[0], 1))
            X = np.concatenate((ones_column, X), axis=1)
        if self.coef_ is None:
            raise Exception('Model has not been trained yet')
        X = np.array(X)
        return X.dot(self.coef_)

    def compute_gradient(self, penalty):
        # Compute the analytical gradient (in vectorized form) of the
        # 1. unregularized mse_loss, 2. mse_loss with ridge regularization
        # or 3. mse_loss with L1 regularization
        if penalty not in ['l2', 'unregularized']:
            raise ValueError('Invalid penalty specified')

        # Compute the gradient of the unregularized MSE loss
        y_pred = self.predict(self.X)
        error = y_pred - self.y
        grad = self.X.T.dot(error) / len(self.y)

        # Add regularization term to the gradient
        if penalty == 'l2':
            grad += self.ridge_lambda * 2 * (self.coef_/len(self.y))

        return grad

    def compute_jax_gradient(self, penalty):
        # Compute the gradient of the
        # 1. unregularized mse_loss,
        # 2. mse_loss with LASSO regularization and
        # 3. mse_loss with ridge regularization, using JAX
        # penalty :  specifies the regularization used , 'l1' , 'l2' or unregularized
        if penalty not in ['l1', 'l2', 'unregularized']:
            raise ValueError('Invalid penalty specified')

        def loss_fn(weights):
            y_pred = jnp.dot(self.X, jnp.asarray(weights))
            mse = jnp.mean((self.y - y_pred)**2)
            return mse

        if penalty == 'unregularized':
            # Compute the gradient of the unregularized MSE loss using JAX
            grad_fn = jax.grad(loss_fn)
            grad = grad_fn(self.coef_)
            return grad

        # Define the regularization penalty function based on the specified penalty
        if penalty == 'l1':
            def reg_fn(weights):
                return self.lasso_lambda * jnp.sum(jnp.abs(weights))
        elif penalty == 'l2':
            def reg_fn(weights):
                return self.ridge_lambda * jnp.sum(weights**2)
        # Compute the gradient of the regularized MSE loss using JAX

        grad_fn = jax.grad(lambda weights: loss_fn(weights) + reg_fn(weights))
        grad = grad_fn(self.coef_)
        return grad

    pass

    def fit_gradient_descent(self, X, y, gradient_type, penalty_type):
        # Implement batch gradient descent for linear regression (should unregularized as well as 'l1' and 'l2' regularized objective)
        # batch_size : Number of training points in each batch
        # num_iters : Number of iterations of gradient descent
        # lr : Default learning rate
        # gradient_type : manual or JAX gradients
        # penalty_type : 'l1', 'l2' or unregularized
        if gradient_type not in ['manual', 'jax']:
            raise ValueError('Invalid gradient_type specified')
        if penalty_type not in ['l1', 'l2', 'unregularized']:
            raise ValueError('Invalid penalty_type specified')

        if(self.fit_intercept):
            ones_column = np.ones((X.shape[0], 1))
            X = np.concatenate((ones_column, X), axis=1)

        n_samples, n_features = X.shape
        n_batches = int(np.ceil(n_samples / self.batch_size))
        self.coef_ = np.random.randn(n_features)
        self.all_coef.append(self.coef_)
        for i in range(self.epochs):
            # Shuffle data and split into batches
            # Shuffle X and y together
            xy = pd.concat([pd.DataFrame(X), pd.Series(y)], axis=1)
            xy_shuffled = xy.sample(frac=1).reset_index(drop=True)

            # Split back into X and y
            X_s = xy_shuffled.iloc[:, :-1]
            y_s = xy_shuffled.iloc[:, -1]

            # Create batches
            batches_X = np.array_split(X_s, n_batches)
            batches_y = np.array_split(y_s, n_batches)

            # Iterate over batches
            for j in range(n_batches):
                # Compute gradient for the current batch

                if gradient_type == 'manual':
                    self.X = batches_X[j]
                    self.y = batches_y[j]
                    grad = self.compute_gradient(penalty_type)
                elif gradient_type == 'jax':
                    self.X = jnp.array(batches_X[j])
                    self.y = jnp.array(batches_y[j])
                    grad = self.compute_jax_gradient(penalty_type)

                # Update coefficients using the gradient
                self.coef_ -= self.lr * grad
                self.all_coef.append(self.coef_)
        self.X = pd.DataFrame(X)
        self.y = pd.Series(y)
        pass

    def fit_SGD_with_momentum(self, X, y, penalty='l2'):
        # Solve the linear regression problem using sklearn's implementation of SGD
        # penalty: refers to the type of regularization used (ridge)

        if(self.fit_intercept):
            ones_column = np.ones((X.shape[0], 1))
            X = np.concatenate((ones_column, X), axis=1)

        n_samples, n_features = X.shape
        n_batches = n_samples
        weights = np.random.randn(n_features)
        self.coef_ = weights
        self.all_coef.append(weights)
        velocity = 0

        # Train the model using stochastic gradient descent with momentum and ridge regularization
        for epoch in range(self.epochs):
            xy = pd.concat([pd.DataFrame(X), pd.Series(y)], axis=1)
            xy_shuffled = xy.sample(frac=1).reset_index(drop=True)

            # Split back into X and y
            X_s = xy_shuffled.iloc[:, :-1]
            y_s = xy_shuffled.iloc[:, -1]

            # Create batches
            batches_X = np.array_split(X_s, n_batches)
            batches_y = np.array_split(y_s, n_batches)

            for i in range(len(X)):
                # Compute the gradient of the loss function w.r.t. weights

                X_sgd = batches_X[i]
                y_sgd = batches_y[i]

                if penalty == 'l1':
                    self.X = jnp.array(X_sgd)
                    self.y = jnp.array(y_sgd)
                    grad = self.compute_jax_gradient(penalty)
                else:
                    self.X = X_sgd
                    self.y = y_sgd
                    grad = self.compute_gradient(penalty)

                # Update the weights using stochastic gradient descent with momentum

                velocity = self.momentum*velocity + (1-self.momentum)*grad
                weights = weights - self.lr*velocity
                self.coef_ = weights
                self.all_coef.append(weights)
        # Store the weights
        self.coef_ = weights
        self.X = pd.DataFrame(X)
        self.y = pd.Series(y)

        pass

    def plot_line_fit(self, X, y, theta_0, theta_1, ax):
        """
        Function to plot fit of the line (y vs. X plot) based on chosen value of theta_0, theta_1. Plot must
        indicate theta_0 and theta_1 as the title.
          :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
          :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
          :param theta_0: Value of theta_0 for which to plot the fit
          :param theta_1: Value of theta_1 for which to plot the fit
          :return matplotlib figure plotting line fit
        """
        # Make predictions for the given theta_0 and theta_1
        self.coef_ = np.array([theta_0, theta_1])
        y_pred = self.predict(X)

        # Create the plot
        ax.clear
        # fig, ax = plt.subplots()
        ax.scatter(X, y, color='blue')
        ax.plot(X, y_pred, color='red')
        ax.set_xlabel('X')
        ax.set_ylabel('y')
        ax.set_title('Linear Regression Fit')

        # return fig
        # pass

    def plot_contour(self, X, y, theta_0, theta_1, ax):
        """
        Plots the RSS as a contour plot. A contour plot is obtained by varying
        theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1, and the
        direction of gradient steps. Uses self.coef_ to calculate RSS.
          :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
          :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
          :param theta_0: Value of theta_0 for which to plot the fit
          :param theta_1: Value of theta_1 for which to plot the fit
          :return matplotlib figure plotting the contour
        """
        theta_0_range = np.linspace(-20, 20, 500)
        theta_1_range = np.linspace(-20, 20, 500)
        rss = np.zeros((len(theta_0_range), len(theta_1_range)))
        for i, theta_0_val in enumerate(theta_0_range):
            for j, theta_1_val in enumerate(theta_1_range):
                self.coef_ = np.array([theta_0_val, theta_1_val])
                y_pred = self.predict(X)
                rss[i, j] = np.sum((y_pred - y)**2)

        # Create the plot
        theta_0_mesh, theta_1_mesh = np.meshgrid(theta_0_range, theta_1_range)
        contours = ax.contour(theta_0_mesh, theta_1_mesh,
                              rss.T, levels=20, cmap='viridis')
        ax.clabel(contours, inline=True, fontsize=8)
        ax.set_xlabel('Theta 0')
        ax.set_ylabel('Theta 1')
        ax.set_title('Residual Sum of Squares Contour Plot')

        ax.plot(theta_0, theta_1, 'ro')

    def plot_surface_init(self, X, y, ax, feature_names=None):
        # if self.fit_intercept:
        #     ones_column = np.ones((X.shape[0], 1))
        #     X = np.concatenate((ones_column, X), axis=1)

        # Set up a grid of theta values
        theta_0_range = np.linspace(-6, 6, 500)
        theta_1_range = np.linspace(-6, 6, 500)
        rss = np.zeros((len(theta_0_range), len(theta_1_range)))
        for i, theta_0_val in enumerate(theta_0_range):
            for j, theta_1_val in enumerate(theta_1_range):
                self.coef_ = np.array([theta_0_val, theta_1_val])
                y_pred = self.predict(X)
                rss[i, j] = np.sum((y_pred - y)**2)

        # Plot the RSS surface
        theta_0_mesh, theta_1_mesh = np.meshgrid(theta_0_range, theta_1_range)
        ax.plot_surface(theta_0_mesh, theta_1_mesh,
                        rss.T, cmap='viridis', alpha=0.5)
        if feature_names:
            ax.set_xlabel(feature_names[0])
            ax.set_ylabel(feature_names[1])
        ax.set_zlabel('RSS')
        ax.set_title('Residual Sum of Squares Surface Plot')
        return rss
